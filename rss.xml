<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>UCCL</title><description>An amazing team!</description><link>https://uccl-project.github.io/</link><language>en-US</language><item><title>Previewing UCCL-EP: Flexible and Efficient Expert Parallelism for Cloud and Beyond</title><link>https://uccl-project.github.io/posts/uccl-ep/</link><guid isPermaLink="true">https://uccl-project.github.io/posts/uccl-ep/</guid><description>GPU-driven communication (e.g., DeepEP) is the key to efficient and large-scale EP, but it cannot run on heterogeneous platforms in the public cloud due to tight coupling between GPU and NIC.</description><pubDate>Mon, 27 Oct 2025 00:00:00 GMT</pubDate><content:encoded>**By: Ziming Mao, Yang Zhou, Yihan Zhang, Chihan Cui, Zhiying Xu, and other UCCL-EP contributors -- Oct 27, 2025**

&lt;div class=&quot;tldr&quot;&gt;
&lt;p&gt;
GPU-driven communication (e.g., DeepEP) is the key to efficient and large-scale EP, but it cannot run on heterogeneous platforms in the public cloud due to tight coupling between GPU and NIC. UCCL-EP has exactly the same interface and functionality as DeepEP, but allows you to run GPU-driven communication for MoE models on public clouds, such as AWS, with superior performance to the state-of-the-art. Our ultimate goal with UCCL-EP is to democratize EP for heterogeneous GPUs and NIC vendors, including AMD GPUs, Broadcom NICs, AMD Pensando NICs, and more. UCCL-EP open-source: &lt;a href=&quot;https://github.com/uccl-project/uccl/tree/main/ep&quot;&gt;uccl-project/uccl/ep&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;

## Expert Parallelism (EP)

**Expert Parallelism (EP)** is widely used in large-scale Mixture-of-Experts (MoE) models, where different subsets of the model’s “experts” are placed on different GPUs across multiple nodes. During inference or training, each input token is routed—based on a learned gating function—to one or a few selected experts. 

This selective routing requires frequent **dispatch** (sending token embeddings to the correct expert GPUs) and **combine** (gathering expert outputs back to their original positions) operations across the network. These data exchanges are typically performed using Remote Direct Memory Access (RDMA) over high-speed interconnects such as InfiniBand or RoCE.

Unlike traditional data or tensor parallelism, where communication involves large contiguous tensors (on the order of megabytes or gigabytes), EP communication is highly **fine-grained**. Each dispatch or combine operation often involves **small message sizes**—for example, 7 KB to 256 KB in systems like **DeepSeek V3**. Such small message sizes pose a challenge for **general-purpose collective communication libraries** like NCCL, which are optimized for high-throughput transfers of large payloads (e.g., in all-reduce or all-gather operations). When messages are this small, the per-transfer latency and synchronization overhead dominate, leading to poor utilization of network bandwidth. Consequently, EP systems often require **custom, low-latency communication runtimes** that can overlap computation and communication efficiently and handle a large number of concurrent small-message operations.

One popular library for EP is **DeepEP**, which leverages NVIDIA-specific NVSHMEM/IBGDA techniques to let NVIDIA GPUs directly issue RDMA operations to NVIDIA NICs for small-message efficiency. IBGDA essentially runs the NIC driver functions inside the GPU SM cores, so that the GPUs can talk to NICs, bypassing the CPU. The GPU can thus enqueue RDMA writes, reads, or atomic operations straight to the NIC’s doorbell registers. However, while DeepEP has high performance, it suffers from two limitations caused by such **tight coupling between GPUs and NICs**.

---

## Limitations of tightly coupling NIC and GPU

### Lack of Portability

DeepEP is tightly coupled with the **NVIDIA software and hardware ecosystem**. It depends on NVIDIA GPUs, NVIDIA NICs, and their proprietary networking stack (e.g., NVSHMEM, GPUDirect, and IBGDA). As a result, DeepEP can only run on NVIDIA-controlled platforms where these components are co-designed and supported.

This design significantly limits portability. For instance, DeepEP cannot run on **AWS cloud instances**, which use **Elastic Fabric Adapter (EFA)** RDMA NICs instead of NVIDIA hardware. Similar incompatibilities arise on other public clouds and data center environments that deploy non-NVIDIA RDMA solutions, such as **Broadcom Thor NICs**, **Google Cloud Falcon NICs**, and **AMD Pensando NICs**. The same restriction applies to GPU vendors—DeepEP’s reliance on NVIDIA-specific APIs and device driver interfaces makes it difficult, if not impossible, to run on **AMD or Intel GPUs**, even when comparable RDMA-capable networking hardware is present.

This lack of cross-vendor portability increasingly limits deployment flexibility as modern AI clusters become more heterogeneous across GPU architectures and networking fabrics.

### Lack of Control and Visibility

By moving NIC driver logic into GPU threads, DeepEP sacrifices **fine-grained control and observability** over the communication process. In traditional CPU-driven RDMA systems, the host manages **flow control**, **queue depth**, **completion notifications**, and **load balancing across multiple network queues**. These mechanisms are essential for ensuring fairness, congestion avoidance, and recovery under high network pressure.

In the IBGDA model, however, GPUs issue RDMA operations directly without the CPU’s coordination. This makes it difficult to monitor or regulate traffic. For example, the GPU may post many outstanding RDMA writes without global awareness of NIC queue utilization, leading to congestion or dropped completions. Detecting transfer completion or handling network backpressure is also not possible in DeepEP, as IBGDA or NVSHMEM does not expose relevant interfaces.

---

## Previewing UCCL EP

UCCL-EP directly tackles these tight-coupling issues and proposes a flexible yet efficient EP solution for the public cloud and heterogeneous device vendors, including GPU and NICs. UCCL-EP preserves the same APIs as DeepEP, supporting both the low latency and normal mode.

The core insight of UCCL-EP is that efficient expert-parallel communication, while benefiting from GPU **initiation**, does not require GPUs to **directly** control the NIC. Instead, UCCL-EP restores a clean separation of concerns between compute and control:

- GPUs retain their massive parallelism for data-intensive tasks — such as token packing, expert combination, NVL forwarding, and local RDMA buffering, and efficient overlap with the background RDMA communication.
- CPUs handle the control-intensive aspects of networking — including queue management, flow control, completion handling, and load balancing — through a lightweight, multi-threaded CPU proxy.

Essentially, **UCCL-EP decouples GPU computation from direct NIC control**. Instead of having GPUs post RDMA operations directly to the NIC (as in NVIDIA’s IBGDA model), each GPU forwards **lightweight control commands**—such as “write this tensor to peer X”—to the CPU through a high-speed shared memory channel. A pool of **multi-threaded CPU proxies** then interprets these commands and issues the actual RDMA verbs to the NIC on the GPU’s behalf.

We note that UCCL-EP’s approach shares similarity with NVSHMEM’s IBRC solution that uses CPU proxies as well, but differs from them by leveraging multiple CPU proxy threads for performance, and supporting a wide range of vendors for portability.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/uccl-project/uccl-project.github.io/main/assets/uccl-ep/ep-illustration.png&quot; alt=&quot;UCCL-EP illustration&quot; width=&quot;600&quot;/&gt;
  &lt;em&gt;Figure 1: RDMA commands initiated by the GPU are handed off to multiple CPU proxy threads. 
&lt;/em&gt;
&lt;/p&gt;

This design exploits a key observation: every RDMA NIC already exposes a standardized, vendor-neutral interface via the **libibverbs** library, maintained by the Linux-RDMA community. By having GPUs forward RDMA requests to CPU threads over PCIe, UCCL-EP can issue network operations on behalf of GPUs using the same verbs API that any NIC driver supports.

The second observation underlying UCCL-EP’s design is that **CPU–GPU communication latency is no longer the dominant bottleneck**. Modern interconnects such as PCIe Gen5, NVLink, and emerging C2C (chip-to-chip) links offer microsecond-scale latency and tens to hundreds of GB/s bandwidth between CPUs and GPUs. This means that forwarding a control command from the GPU to the CPU is extremely fast—especially compared to the end-to-end latency of an RDMA operation that traverses the network.

Moreover, each control command in expert parallelism typically represents a **batched data movement involving multiple tokens** (e.g., a dispatch or combine operation that transfers tens or hundreds of kilobytes). Therefore, the amortized cost of sending a command descriptor over PCIe is negligible relative to the data volume it represents. In other words, UCCL-EP uses the CPU for **coarse-grained control**, ensuring that the PCIe path is well within the performance envelope of small-message MoE workloads.

---

## Designing an efficient CPU-GPU communication channel

A central challenge in UCCL-EP is building an efficient **forwarding channel between GPUs and CPUs** that can sustain tens of millions of RDMA requests per second without becoming a bottleneck. UCCL-EP implements this channel as a carefully optimized **lock-free FIFO queue** shared between GPU producers and CPU consumers. Each GPU enqueues lightweight RDMA transfer descriptors into the queue, while multiple CPU threads dequeue and execute them through libibverbs.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/uccl-project/uccl-project.github.io/main/assets/uccl-ep/ep-fifo.png&quot; alt=&quot;UCCL-EP FIFO illustration&quot; width=&quot;400&quot;/&gt;
  &lt;em&gt;Figure 2: UCCL-EP employs multiple channels per GPU; The tail is read by CPU thread and allocated on host, the head is read and updated by GPU thread and allocated on device. It further caches the tail value on GPU for faster access. Each TransferCmd is small.  
&lt;/em&gt;
&lt;/p&gt;

UCCL-EP employs multiple channels per GPU; The tail is read by CPU thread and allocated on host, the head is read and updated by GPU thread and allocated on device. It further caches the tail value on GPU for faster access. Each TransferCmd is small. 

This careful design allows each GPU to achieve over **50 million RDMA operations** per second with modest latency overhead (as shown in UCCL PR [#454](https://github.com/uccl-project/uccl/pull/454)), where the NIC’s intrinsic latency and network delay—not the CPU–GPU channel—becomes the dominant cost. Recent hardware trends make such hybrid coordination increasingly practical—systems like NVIDIA GH200, GB200/GB300, and the NVIDIA-Intel NVLink-C2C partnership all feature high-bandwidth, low-latency CPU–GPU interconnects that rival traditional NUMA memory links.

---

## Working with various GPU-NIC vendors

Different **NIC vendors** introduce additional system-level challenges due to variations in transport protocols and hardware capabilities. For instance, AWS EFA NICs use the **Scalable Reliable Datagram (SRD)** protocol, which employs advanced **multi-pathing** to mitigate congestion at scale. While this design improves throughput and reliability, it breaks the strict in-order delivery guarantee within a single SRD Queue Pair (QP). This becomes problematic for DeepEP-style communication, which relies on ordered RDMA writes followed by atomic operations to notify remote GPUs that the data has been written into its HBM.

To address this, **UCCL-EP** leverages its CPU-side flexibility to enforce **software-level message ordering**. Each RDMA write carries **immediate data** encoding a per-channel sequence number, which the receiver uses to **reorder out-of-sequence messages** before committing them to GPU memory. 

Furthermore, In DeepEP’s NVIDIA-specific IBGDA path, GPUs rely on **hardware RDMA atomics** to signal remote completion. However, **EFA does not natively support RDMA atomics**, which poses a correctness challenge: the receiver must still know when a payload has been fully written before it can proceed to read or combine it.

To emulate this behavior, UCCL-EP implements **software-level atomics** using regular RDMA writes and immediate data. The sender writes the payload first, then issues a small RDMA write carrying an immediate value that acts as an atomic message (e.g., the new counter value or flag). On the receiver side, the CPU proxy updates a local completion counter — effectively reproducing the synchronization semantics of hardware atomics. 

To enable UCCL EP work with diverse GPU vendors, we have taken the first step in eliminating nvshmem dependencies, which is important for portability as well as other features (e.g. elastic scaling). We also observed interestingly, removing nvshmem dependency can sometimes lead to performance improvements, which we suspect to be due to the internal overhead of the nvshmem library. 

---

## Performance

On EFA, we observe UCCL-EP significantly outperforms other baselines as we increase the number of tokens in the dispatch and combine. We used unmodified [Perplexity MoE Kernels](https://github.com/perplexityai/pplx-kernels/tree/master) and ran on H200 with EFA NICs. For the NVSHMEM and Torch baselines, we wrote an efficient packing and unpacking kernel, and relied on their respective AlltoAll APIs to distribute packed tokens to destination ranks in a single contiguous transfer. 

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/uccl-project/uccl-project.github.io/main/assets/uccl-ep/ep-efa.png&quot; alt=&quot;UCCL-EP EFA results&quot; width=&quot;500&quot;/&gt;
  &lt;em&gt;Figure 3: On 2 nodes, H200 + EFA (400 Gbps)
&lt;/em&gt;
&lt;/p&gt;

We test normal kernels on H200 (8× GPUs per node) with each node connected to an EFA 400 Gb/s RDMA network card. We follow the DeepSeek-V3 pretraining configuration (4096 tokens per batch, 7168 hidden, top-4 groups, top-8 experts, FP8 dispatch and BF16 combine).

| Type | Dispatch FP8 #EP | Bottleneck bandwidth | Combine BF16 #EP | Bottleneck bandwidth |
|:---------:|:------------:|:--------------------:|:-----------:|:--------------------:|
| Intranode | 8 | 320 GB/s (NVLink) | 8 | 319 GB/s (NVLink) |
| Internode | 16 | 50 GB/s (RDMA) | 16 | 18 GB/s (RDMA) |
| Internode | 24 | 53 GB/s (RDMA) | 24 | 26 GB/s (RDMA) |
| Internode | 32 | 54 GB/s (RDMA) | 32 | 43 GB/s (RDMA) |

Across different EP sizes, the dispatch bandwidth exceeds 50 GB/s, while the combine bandwidth stabilizes around 40 GB/s. The slightly lower combine bandwidth reflects the additional overhead of the combine operation (e.g., accumulation and reduction across experts). We are still investigating the relatively lower combine throughput compared to dispatch at EP=16. 

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/uccl-project/uccl-project.github.io/main/assets/uccl-ep/ep-gh200.png&quot; alt=&quot;UCCL-EP EFA results&quot; width=&quot;600&quot;/&gt;
  &lt;em&gt;Figure 4: On 2 nodes, GH200 + CX7 (200 Gbps).
&lt;/em&gt;
&lt;/p&gt;

On a small testbed with GH200, we observe that UCCL-EP even outperforms the original DeepEP. We are surprised by the results, and hypothesize two reasons: the fast NVLink-C2C interconnect with **CPU-GPU cache coherence** on GH200 makes CPU-GPU communication very efficient; and the internal overhead of nvshmem. That said, we would like to verify the finding on larger testbeds. 

Benchmark code and instructions can be found here:  
https://github.com/uccl-project/uccl/tree/main/ep#benchmark 

---

## UCCL EP roadmap

UCCL-EP is still in active development. We plan to release a formal post on application-level performance as well as performance on AMD GPUs and other NIC vendors. Our current roadmap includes:

- Finishing porting to AMD GPUs and Broadcom NICs  
- Advanced flow control and congestion management in the CPU  
- Integrating into vLLM and SGLang—contributions are much welcomed! 

---

## Acknowledgements

We thank AWS, Lambda Labs for providing us with the main testbeds. This research is supported by gifts from Accenture, AMD, Anyscale, AWS, Broadcom, Cisco, Google, IBM, Intel, Intesa Sanpaolo, Lambda, Lightspeed, Mibura, Microsoft, NVIDIA, Samsung SDS, and SAP.</content:encoded></item><item><title>Everything You Want to Know about KV Cache Transfer Engine</title><link>https://uccl-project.github.io/posts/kv-transfer-engine/</link><guid isPermaLink="true">https://uccl-project.github.io/posts/kv-transfer-engine/</guid><description>There have been many KV cache transfer engines for PD disaggregation, but nearly no benchmarks on their performance. This blog serves for this purpose---benchmarking and analyzing the performance of various KV transfer engines, so you can decide how to choose.</description><pubDate>Wed, 13 Aug 2025 00:00:00 GMT</pubDate><content:encoded>**By: UCCL Team -- Aug 13, 2025**

KV cache transfer is becoming more and more important for modern disaggregated LLM serving with prefill and decode separation. It typically transfers the KV cache generated by the prefill GPU node to the separate decode GPU node via fast RDMA networks. Therefore, the KV cache transferring speed directly impacts the end-to-end request latency. 

There have been quite a few KV cache transfer engines in the open-source world, including [NCCL](https://github.com/NVIDIA/nccl)/[RCCL](https://github.com/ROCm/rccl) (for AMD), [Nvidia NIXL](https://github.com/ai-dynamo/nixl), and [Mooncake TE](https://github.com/kvcache-ai/Mooncake) (Transfer Engine), but nearly no benchmarks on their performance. This blog serves for this purpose—benchmarking and analyzing the performance of various KV transfer engines, so you can decide how to choose. 

We also want to introduce the UCCL KV transfer engine called [UCCL P2P](https://github.com/uccl-project/uccl/tree/main/p2p). As we will show soon, it provides an easy-to-use collective APIs like NCCL/RCCL, but provides much better performance than NCCL/RCCL without burning any SMs for the scenario of peer-to-peer KV cache transfer. So in total, this blog will compare the performance of the following four transfer engines: NCCL/RCCL, NIXL, Mooncake TE, and UCCL P2P.

## Transfer engine designs

**NCCL/RCCL** were mainly used for collective communication, like allreduce in LLM training and inference workloads. NCCL and RCCL have almost the same design and codebase, except that they were built for Nvidia and AMD GPUs, respectively. It provides flexible send/recv P2P transfer APIs that can be used for KV cache transfer. NCCL/RCCL always requires launching a GPU kernel even when doing `send/recv` P2P transfers. Therefore, it will consume some GPU SM resources during the transfer process. These GPU SMs are mainly used for data copy and reduce operations in complex collectives, but should be unnecessary during P2P transfer. 

**NIXL** was created by Nvidia Dynamo distributed LLM inference framework, as its KV transfer solution. It has a modular design with a bunch of transfer backends, including file systems, POSIX sockets, and RDMA networks. It has several RDMA network backends: one is based on a famous communication library from the HPC community, called [UCX](https://github.com/openucx/ucx); another is exactly the Mooncake TE that we will benchmark soon. One thing worth noting is that UCX supports AMD GPUs. NIXL provides vastly different APIs from the NCCL/RCCL, with `read/write` operations between KV cache exporter nodes and importer nodes. A node needs to export some metadata for its KV cache using an out-of-band network, such as a TCP socket or an ETCD service endpoint, so that the other nodes can read from or write to the KV data. It does not consume GPU SM resources, as the KV cache is directly transferred in a GPU-Direct RDMA manner.  

**Mooncake TE** is a component of the serving platform for Kimi from Moonshot AI. It also has different APIs other than NCCL/RCCL, very similar to NIXL style of `read/write`. It leverages GPU-Direct RDMA to directly transfer the KV cache without consuming GPU SM resources. It has a cool feature of automatically detecting the NIC and GPU affinity from PCIe topology, so that applications do not need to specify which NIC to use for each GPU. However, based on our testing, it has not supported AMD GPUs so far. 

**UCCL P2P** is our recently developed KV cache transfer engine that features SM-freeness, a light-weight codebase, and easy-to-use interfaces. It is based on UCCL’s multi-path RDMA transport engine, provides both `read/write` and collective `send/recv` APIs without burning any GPU SMs, and auto-detect GPU-NIC topology. Its collective APIs avoid the need for an explicit out-of-band network for metadata dispersing. You can check our collective APIs [here](https://github.com/uccl-project/uccl/blob/bde34635dfeef86faa43b4bc3ccfcbc92a20aeaf/p2p/collective.py#L473-L491). 

## Benchmarking setup

Our benchmark testbed consists of two AMD GPU nodes, each with 8 MI300X GPUs and 8 Broadcom Thor-2 400Gbps NICs (or 50GB/s). We use the first GPU of each node with its most-affinitive NIC to benchmark P2P transfers. We use a wide range of message sizes, ranging from a few KB to dozens of MB, to reflect the possible KV cache size in practice. We measure the end-to-end latency of each **single** message transfer and calculate its achieved bandwidth. For the Mooncake engine, since it does not support AMD GPUs, we use host DRAM for KV cache transfer. We have double checked that for NIXL and UCCL P2P, using host DRAM won’t change their performance for our targeted single-direction two-GPU transfer scenario. 

For Mooncake, we use its official C++ benchmark [`transfer_engine_bench.cpp`](https://github.com/kvcache-ai/Mooncake/blob/main/mooncake-transfer-engine/example/transfer_engine_bench.cpp); for others, we use their Python APIs to implement simple KV cache transfer [benchmarks](https://github.com/uccl-project/uccl/tree/main/p2p/benchmarks). 

## Benchmarking results

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/uccl-project/uccl-project.github.io/main/assets/kv-transfer-engine/p2p-single-direction.png&quot; alt=&quot;P2P single direction&quot; width=&quot;600&quot;/&gt;
  &lt;em&gt;Figure 1: Throughput comparison of different KV cache transfer engines.&lt;/em&gt;
&lt;/p&gt;

The above figure shows the benchmarking results with NIXL and UCCL P2P achieving the best performance for almost all message sizes. NCCL/RCCL performs roughly **30-50%** slower than NIXL and UCCL P2P for 256KB and 1MB message sizes typically seen in KV cache transfer scenarios. But NCCL/RCCL’s performance reaches or even surpasses NIXL and UCCL P2P after 10MB. We think this might be because of NCCL/RCCL-internal smart message chunking mechanisms. 

Surprisingly, we find Mooncake TE performs the worst, not able to saturate 50GB/s network link even at 100MB. We failed to find a architecture-level issues for it, but can only attribute it to the potentially buggy implementation. One thing to note is that we find by tuning the number of task submission threads in the Mooncake `transfer_engine_bench.cpp`, it can achieve 50GB/s line rate, but this is in contrast to other engine’s ability of reaching line rate with only one task submission thread. 

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/uccl-project/uccl-project.github.io/main/assets/kv-transfer-engine/p2p-dual-direction.png&quot; alt=&quot;P2P dual direction&quot; width=&quot;600&quot;/&gt;
  &lt;em&gt;Figure 1: Throughput comparison for dual-direction transfers.&lt;/em&gt;
&lt;/p&gt;

We also perform another interesting benchmark that does KV transfer between two GPUs in dual directions to stress test the engine transfer performance. The results are shown above, where we find that NCCL/RCCL starts to see SM being the transfer bottlenecks. UCCL P2P performs slightly better than NIXL at the 256KB message size. 

## Comparison summary

|             | Throughput | API                      | SM-free | AMD support | Auto-detect topo |
|-------------|------------|--------------------------|---------|-------------|---------------------|
| NCCL/RCCL   | Medium     | Collective               | No      | Yes         | Yes                 |
| NIXL        | High       | Read/Write               | Yes     | Yes         | No                  |
| Mooncake TE | Low        | Read/Write               | Yes     | No          | Yes                 |
| UCCL P2P    | High       | Collective +&lt;br&gt;Read/Write | Yes     | Yes         | Yes                 |


## Last words on UCCL P2P

UCCL P2P is rapidly evolving, and we would love to hear your feedback and contributions. Our next goal is to extend our send/recv collective to more types of collectives without burning any GPU SM resources, to accelerate distributed video generation and understanding scenarios. So talk to us and checkout our GitHub repository (https://github.com/uccl-project/uccl) if you are interested in how UCCL P2P can help accelerate these emerging scenarios. Stay tuned!</content:encoded></item><item><title>How to Debug NCCL Performance Issues for ML Workloads?</title><link>https://uccl-project.github.io/posts/debug-nccl/</link><guid isPermaLink="true">https://uccl-project.github.io/posts/debug-nccl/</guid><description>NCCL is notoriously hard to debug. In this post, we will go through our journey of debugging NCCL performance issues and how UCCL can help this process.</description><pubDate>Mon, 30 Jun 2025 00:00:00 GMT</pubDate><content:encoded>**By: UCCL Team -- June 30, 2025**

NCCL is the cornerstone in modern distributed machine learning workloads, connecting dozens to thousands of GPUs for synchronous collective communication. Any small performance problem happening in any GPU or any NIC would cause the overall communication slowdown, increasing the job completion time. 

Running NCCL performantly in public cloud GPUs is challenging, mainly because of two reasons: 1) NCCL has so many environmental variables (up to 78 if you check [NCCL‘s doc](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#system-configuration))---there are even some variables hidden inside the NCCL code not exposed out, and 2) networking layers such as RDMA can sometimes cause performance issues, while we users have very little control. 

## A NCCL Debugging Experience 

A few weeks ago, we got access to six HGX VMs from a major public cloud provider. Each of the VMs has 8 H100 GPUs and 8 400Gbps Nvidia CX-7 NICs; these VMs are across two racks in a multi-tier RoCE network. We set up NCCL v2.23.4 and run all-reduce from NCCL-tests (9d26b84) across these six VMs. However, we got surprisingly low performance in our first attempt. It is far lower than what others have measured of 360GB/s bus bandwidth (busbw) [^1]. Our experience told us it is more likely to be the inter-node networking issues, especially as the intra-node NVLink gives good results when running collective within a single node. 

We decided to only look at the inter-node networking issues by setting two environmental variables of NCCL: `NCCL_P2P_DISABLE=1, NCCL_SHM_DISABLE=1`. These two environment variables will enforce the NCCL collectives to only use the inter-node network, but not any intra-node networks like NVLink or local host shared memory. After disabling intra-node networks, we find that all-reduce is only able to achieve half of the NIC bandwidth, which is around 23GB/s (the NIC has 50GB/s bandwidth). 

We then guess it might be that the NIC has two ports, while NCCL only uses one port? Following this guess, we try to increase the number of RDMA QPs so that the two ports can be fully utilized. We set the environmental variable `NCCL_IB_QPS_PER_CONNECTION` to 4. You can also use another hidden variable `NCCL_NCHANNELS_PER_NET_PEER` that should give a similar effect. With this setting, now NCCL is able to go beyond 25GB/s, but still far behind theoritical 50GB/s. 

So what happens? Our next guess is that NCCL would require a larger number of SMs to catch up with the fast network. This number can be tuned by setting the number of NCCL channels: `NCCL_MAX_NCHANNELS=8, NCCL_MIN_NCHANNELS=8`. With this setting, we thought we should be able to reach 50GB/s; unfortunately, NCCL performance saturates at around 39GB/s. Furthermore, we find that NCCL busbw gets severe drops at large message sizes like 128MB. 

What could be the possible cause of this low performance and bus bandwidth (busbw) drop at large messages? We finally guess it could be the RoCE network congestion in a typical datacenter networking topology Fattree [^2]. When multiple network flows come out of a rack switch to another rack switch, these flows may map to the same upper switch port and cause network congestion. Recall that each of the two flows could have up to 400Gbps traffic rate in ML workload, while a single switch port only has 400Gbps. If so, how are we gonna resolve this? Especially, we do not have any control over the NIC and switch settings in the public cloud.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/uccl-project/uccl-project.github.io/main/assets/nccl-debug/ecmp_collision.png&quot; alt=&quot;ECMP hash collision&quot; width=&quot;600&quot;/&gt;
  &lt;em&gt;Figure 1: Network congestion could happen as each RDMA flow only goes through a single and fixed network path in RoCE, and two flows could collide on the same switch port.&lt;/em&gt;
&lt;/p&gt;

The final solution is that we leverage UCCL by simply setting another environmental variable `NCCL_NET_PLUGIN=libnccl-net-uccl.so`. The high-level idea here is that UCCL will leverage hundreds of network paths in a congestion-aware way to avoid the network congestion issues, therefore reaching the NIC bandwidth limit of 50GB/s. Another good thing about UCCL is that it does this in a transparent way, without modifying any piece of application code or even NCCL code. 

Finally, with UCCL, we are able to avoid the NCCL performance drop at large messages and further improve its maximum throughput. 

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/uccl-project/uccl-project.github.io/main/assets/nccl-debug/nccl_vs_uccl.png&quot; alt=&quot;NCCL vs UCCL&quot; width=&quot;600&quot;/&gt;
  &lt;em&gt;Figure 2: All-reduce performance across six machines, each with 8 H100 GPUs and 8 400G Nvidia NICs across two racks. Note that this measurement only uses 8 GPU SMs for NCCL/UCCL to leave enough SMs for ML applications; the measurement has NVLS off because of the GPU VM’s configuration issues on nv-fabricmanager.&lt;/em&gt;
&lt;/p&gt;

### The Ultimate List

Based on UCCL team&apos;s experience, here we summarize a list of highly relevant NCCL parameters that are worthy of tuning when hitting NCCL performance issues. Note that the list also includes some parameters we have not had time to cover.  
* Looking at the networking performance: `NCCL_P2P_DISABLE=1, NCCL_SHM_DISABLE=1`
* Number of QPs: `NCCL_IB_QPS_PER_CONNECTION, NCCL_NCHANNELS_PER_NET_PEER`.
* Number of channels: `NCCL_MAX_NCHANNELS, NCCL_MIN_NCHANNELS`
* Transport buffer and chunk sizes: `NCCL_P2P_NET_CHUNKSIZE=524288, NCCL_BUFFSIZE=8388608`
  * These are extremely useful for AWS GPU VMs that use AWS EFA RDMA NICs
* PCIe relaxed ordering: `NCCL_IB_PCI_RELAXED_ORDERING`
  * This could be useful when using GPU VMs rather baremetal machine
* Using the UCCL plugin: `NCCL_NET_PLUGIN=libnccl-net-uccl.so`


## Another Debugging Experience on AMD GPUs

Last week, we got access to a production cluster with AMD MI300X GPU + Broadcom 400G NICs, **without** any root privileges. We observe a similar performance drop for RCCL (AMD’s collective communication library) at large messages. We tried the UCCL plugin, and it effectively brought performance back. 

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/uccl-project/uccl-project.github.io/main/assets/nccl-debug/rccl_vs_uccl.png&quot; alt=&quot;RCCL vs NCCL&quot; width=&quot;600&quot;/&gt;
  &lt;em&gt;Figure 3: All-to-all performance across two machines, each with 4 AMD MI300X GPUs and 4 400G Broadcom NICs.&lt;/em&gt;
&lt;/p&gt;

## Summary

Debugging NCCL/RCCL performance is challenging, not only due to complex and hidden configurations, but also because of uncontrollable networking behaviors in public cloud deployments. Our UCCL team is here to help. We provide free consulting on performance-related NCCL/RCCL issues and the UCCL plugin to help you get rid of any network congestion issues. 

UCCL is fully open-source at https://github.com/uccl-project/uccl, with many developers and maintainers from UC Berkeley Sky Computing Lab, the lab that once created Spark, Ray, and vLLM. We enthusiastically invite open-source developers to join and contribute to the UCCL project. 

---

[^1]: https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/#multi-node-rcclnccl-collectives-and-scale-out-network-benchmarks. 
[^2]: Al-Fares, Mohammad, Alexander Loukissas, and Amin Vahdat. &quot;A scalable, commodity data center network architecture.&quot; ACM SIGCOMM computer communication review 38.4 (2008): 63-74. [Paper link](http://ccr.sigcomm.org/online/files/p63-alfares.pdf).</content:encoded></item></channel></rss>