<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>UCCL</title><description>An amazing team!</description><link>https://uccl-project.github.io/</link><language>en-US</language><item><title>UCCL</title><link>https://uccl-project.github.io/posts/about-uccl/</link><guid isPermaLink="true">https://uccl-project.github.io/posts/about-uccl/</guid><description>About UCCL</description><pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate><content:encoded># UCCL: An Extensible Software Transport Layer for GPU Networking.

## Fast-evolving ML workloads outpaces slow-evolving networking.

Machine learning (ML) workloads and their requirements for networking are evolving rapidly. Less than ten years ago, deep neural networks only had millions of parameters, and were trained atop hundreds of CPUs/GPUs with parameter servers or allreduce collective communication. After five years, large language models (LLMs) began to surge with billions of parameters, and were trained atop thousands of more powerful GPUs with multi-level parallelism and diverse collectives like allreduce, allgather, and reduce-scatter. In the recent two years, large-scale LLM serving has become the norm; prefill-decode disaggregation as an efficient serving technique, requires fast peer-to-peer communication. This year, serving Mixture-of-Experts (MoE) models like DeepSeek-V3 became very popular, featuring challenging all-to-all communication among hundreds of GPUs.

However, networking techniques especially the host network transport on RDMA NICs are hard to adapt and evolve to better suit the needs of ML workloads. Essentially, hardware changes are time-consuming and take much longer time than software changes. This can lead to a mismatch between the **application needs** and **existing hardware optimizations**, which often translates into poor performance. 

* Meta has reported that DCQCN — a popular congestion control (CC) algorithm in datacenters supported by RDMA NICs—does not work well for LLM training workloads with low flow entropy and high traffic burstiness. As a result, Meta decided to disable the CC support in NICs and instead implement traffic scheduling at the application layer.
* DeepSeek disabled the CC when running large-scale all-to-all for serving MoE models. However, running a large-scale RDMA network without CC is brittle, as it can lead to deadlocks, head-of-line blocking, and pervasive congestion 
* Alibaba has observed severe performance degradation for collective communication during LLM training. This was due to the high level of flow collisions, which in turn was caused by the RDMA NICs supporting only single-flow/path per connection. To avoid this problem, Alibaba has redesigned the network topology for LLM training using a rail-optimized dual-plane architecture. However, such a redesign is costly to build and maintain. 

## UCCL: a software-only extensible transport layer for GPU networking.




## Key challenges addressed by UCCL.

### How to decouple the data and control paths for existing RDMA NICs?

### How to achieve hardware-level performance for software control path?


## Core UCCL Insights.

### Moving control paths to CPU for more states handling and faster processing compared to wimpy ARM/on-chip cores.


### Harnessing multi-path for avoiding path collision.


## Evaluation.


## Future dev plan.</content:encoded></item></channel></rss>